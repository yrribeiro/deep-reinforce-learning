{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9XaULfDZDvrC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (0.29.1)\n",
            "Requirement already satisfied: pygame in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 2)) (2.5.2)\n",
            "Requirement already satisfied: numpy in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: huggingface_hub in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (0.18.0)\n",
            "Requirement already satisfied: pickle5 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 6)) (0.0.11)\n",
            "Requirement already satisfied: pyyaml==6.0 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 7)) (6.0)\n",
            "Requirement already satisfied: imageio in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 8)) (2.31.5)\n",
            "Requirement already satisfied: imageio_ffmpeg in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 9)) (0.4.9)\n",
            "Requirement already satisfied: pyglet==1.5.1 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 10)) (1.5.1)\n",
            "Requirement already satisfied: tqdm in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 11)) (4.66.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (0.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (6.8.0)\n",
            "Requirement already satisfied: filelock in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2023.9.2)\n",
            "Requirement already satisfied: requests in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (23.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from imageio->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 8)) (10.1.0)\n",
            "Requirement already satisfied: setuptools in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from imageio_ffmpeg->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 9)) (66.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 1)) (3.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ynk/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->huggingface_hub->-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt (line 5)) (2023.7.22)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DaY1N4dBrabi"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7ffb945c5e20>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-7f-Swax_9x"
      },
      "source": [
        "## Import the packages 📦\n",
        "\n",
        "In addition to the installed libraries, we also use:\n",
        "\n",
        "- `random`: To generate random numbers (that will be useful for epsilon-greedy policy).\n",
        "- `imageio`: To generate a replay video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VcNvOAQlysBJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "import pickle5 as pickle\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xya49aNJWVvv"
      },
      "source": [
        "# Part 1: Frozen Lake ⛄ (non slippery version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAvihuHdy9tw"
      },
      "source": [
        "## Create and understand [FrozenLake environment ⛄]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
        "---\n",
        "\n",
        "💡 A good habit when you start to use an environment is to check its documentation\n",
        "\n",
        "👉 https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "\n",
        "---\n",
        "\n",
        "We're going to train our Q-Learning agent **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.\n",
        "\n",
        "We can have two sizes of environment:\n",
        "\n",
        "- `map_name=\"4x4\"`: a 4x4 grid version\n",
        "- `map_name=\"8x8\"`: a 8x8 grid version\n",
        "\n",
        "\n",
        "The environment has two modes:\n",
        "\n",
        "- `is_slippery=False`: The agent always moves **in the intended direction** due to the non-slippery nature of the frozen lake (deterministic).\n",
        "- `is_slippery=True`: The agent **may not always move in the intended direction** due to the slippery nature of the frozen lake (stochastic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaW_LHfS0PY2"
      },
      "source": [
        "For now let's keep it simple with the 4x4 map and non-slippery.\n",
        "We add a parameter called `render_mode` that specifies how the environment should be visualised. In our case because we **want to record a video of the environment at the end, we need to set render_mode to rgb_array**.\n",
        "\n",
        "As [explained in the documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) “rgb_array”: Return a single frame representing the current state of the environment. A frame is a np.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jNxUbPMP0akP"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASNViqL4tZn"
      },
      "source": [
        "You can create your own custom grid like this:\n",
        "\n",
        "```python\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
        "```\n",
        "\n",
        "but we'll use the default environment for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXbTfdeJ1Xi9"
      },
      "source": [
        "### Let's see what the Environment looks like:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZNPG0g_UGCfh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_____OBSERVATION SPACE_____ \n",
            "\n",
            "Observation Space Discrete(16)\n",
            "Sample observation 1\n"
          ]
        }
      ],
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "We5WqOBGLoSm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " _____ACTION SPACE_____ \n",
            "\n",
            "Action Space Shape 4\n",
            "Action Space Sample 0\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXwkI2Magx"
      },
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with 4 actions available 🎮:\n",
        "- 0: GO LEFT\n",
        "- 1: GO DOWN\n",
        "- 2: GO RIGHT\n",
        "- 3: GO UP\n",
        "\n",
        "Reward function 💰:\n",
        "- Reach goal: +1\n",
        "- Reach hole: 0\n",
        "- Reach frozen: 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pFhWblk3Awr"
      },
      "source": [
        "## Create and Initialize the Q-table 🗄️\n",
        "\n",
        "(👀 Step 1 of the pseudocode)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
        "\n",
        "\n",
        "It's time to initialize our Q-table! To know how many rows (states) and columns (actions) to use, we need to know the action and observation space. We already know their values from before, but we'll want to obtain them programmatically so that our algorithm generalizes for different environments. Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HuTKv3th3ohG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are  16  possible states\n",
            "There are  4  possible actions\n"
          ]
        }
      ],
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lnrb_nX33fJo"
      },
      "outputs": [],
      "source": [
        "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable\n",
        "\n",
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atll4Z774gri"
      },
      "source": [
        "## Define the greedy policy 🤖\n",
        "\n",
        "Remember we have two policies since Q-Learning is an **off-policy** algorithm. This means we're using a **different policy for acting and updating the value function**.\n",
        "\n",
        "- Epsilon-greedy policy (acting policy)\n",
        "- Greedy-policy (updating policy)\n",
        "\n",
        "The greedy policy will also be the final policy we'll have when the Q-learning agent completes training. The greedy policy is used to select an action using the Q-table.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "se2OzWGW5kYJ"
      },
      "outputs": [],
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action = np.argmax(Qtable[state][:])\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flILKhBU3yZ7"
      },
      "source": [
        "##Define the epsilon-greedy policy 🤖\n",
        "\n",
        "Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.\n",
        "\n",
        "The idea with epsilon-greedy:\n",
        "\n",
        "- With *probability 1 - ɛ* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).\n",
        "\n",
        "- With *probability ɛ*: we do **exploration** (trying a random action).\n",
        "\n",
        "As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cYxHuckr4LiG"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_num = random.uniform(0,1)\n",
        "  # if random_num > greater than epsilon --> exploitation\n",
        "  if random_num > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # np.argmax can be useful here\n",
        "    action = greedy_policy(Qtable, state)\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW80DealcRtu"
      },
      "source": [
        "## Define the hyperparameters ⚙️\n",
        "\n",
        "The exploration related hyperparamters are some of the most important ones.\n",
        "\n",
        "- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.\n",
        "- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck**, since your agent didn't explore enough of the state space and hence can't solve the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Y1tWn0tycWZ1"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 10000  # Total training episodes\n",
        "learning_rate = 0.7          # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "eval_seed = []               # The evaluation seed of the environment\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05            # Minimum exploration probability\n",
        "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDb7Tdx8atfL"
      },
      "source": [
        "## Create the training loop method\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
        "\n",
        "The training loop goes like this:\n",
        "\n",
        "```\n",
        "For episode in the total of training episodes:\n",
        "\n",
        "Reduce epsilon (since we need less and less exploration)\n",
        "Reset the environment\n",
        "\n",
        "  For step in max timesteps:    \n",
        "    Choose the action At using epsilon greedy policy\n",
        "    Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "    Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "    If done, finish the episode\n",
        "    Our next state is the new state\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IyZaYbUAeolw"
      },
      "outputs": [],
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Reset the environment\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "      # If terminated or truncated finish the episode\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # Our next state is the new state\n",
        "      state = new_state\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwKQ4tUdhGI"
      },
      "source": [
        "## Train the Q-Learning agent 🏃"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DPBxfjJdTCOH"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c93e3a1688f24399bd11c51e36c18f7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeEhUCrc30L"
      },
      "source": [
        "## Let's see what our Q-Learning table looks like now 👀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nmfchsTITw4q"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
              "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
              "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
              "       [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
              "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
              "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
              "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
              "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
              "       [0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Qtable_frozenlake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUrWkxsHccXD"
      },
      "source": [
        "## The evaluation method 📝\n",
        "\n",
        "- We defined the evaluation method that we're going to use to test our Q-Learning agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jNl0_JO2cbkm"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param max_steps: Maximum number of steps per episode\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param Q: The Q-table\n",
        "  :param seed: The evaluation seed array (for taxi-v3)\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in tqdm(range(n_eval_episodes)):\n",
        "    if seed:\n",
        "      state, info = env.reset(seed=seed[episode])\n",
        "    else:\n",
        "      state, info = env.reset()\n",
        "    step = 0\n",
        "    truncated = False\n",
        "    terminated = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      # Take the action (index) that have the maximum expected future reward given that state\n",
        "      action = greedy_policy(Q, state)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jJqjaoAnxUo"
      },
      "source": [
        "## Evaluate our Q-Learning agent 📈\n",
        "\n",
        "- Usually, you should have a mean reward of 1.0\n",
        "- The **environment is relatively easy** since the state space is really small (16). What you can try to do is [to replace it with the slippery version](https://gymnasium.farama.org/environments/toy_text/frozen_lake/), which introduces stochasticity, making the environment more complex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fAgB7s0HEFMm"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6f5127255e0439a8cc6b9b0cde05bcd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean_reward=1.00 +/- 0.00\n"
          ]
        }
      ],
      "source": [
        "# Evaluate our Agent\n",
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FiMqxqVHg0I4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'env_id': 'FrozenLake-v1',\n",
              " 'max_steps': 99,\n",
              " 'n_training_episodes': 10000,\n",
              " 'n_eval_episodes': 100,\n",
              " 'eval_seed': [],\n",
              " 'learning_rate': 0.7,\n",
              " 'gamma': 0.95,\n",
              " 'max_epsilon': 1.0,\n",
              " 'min_epsilon': 0.05,\n",
              " 'decay_rate': 0.0005,\n",
              " 'qtable': array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
              "        [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
              "        [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
              "        [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
              "        [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.9025    , 0.        , 0.81450625],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
              "        [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
              "        [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
              "        [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
              "        [0.        , 0.        , 0.        , 0.        ]])}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = {\n",
        "    \"env_id\": env_id,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"n_training_episodes\": n_training_episodes,\n",
        "    \"n_eval_episodes\": n_eval_episodes,\n",
        "    \"eval_seed\": eval_seed,\n",
        "\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"gamma\": gamma,\n",
        "\n",
        "    \"max_epsilon\": max_epsilon,\n",
        "    \"min_epsilon\": min_epsilon,\n",
        "    \"decay_rate\": decay_rate,\n",
        "\n",
        "    \"qtable\": Qtable_frozenlake\n",
        "}\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18lN8Bz7yvLt"
      },
      "source": [
        "# Part 2: Taxi-v3 🚖\n",
        "\n",
        "## Create and understand [Taxi-v3 🚕](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
        "---\n",
        "\n",
        "💡 A good habit when you start to use an environment is to check its documentation\n",
        "\n",
        "👉 https://gymnasium.farama.org/environments/toy_text/taxi/\n",
        "\n",
        "---\n",
        "\n",
        "In `Taxi-v3` 🚕, there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue).\n",
        "\n",
        "When the episode starts, **the taxi starts off at a random square** and the passenger is at a random location. The taxi drives to the passenger’s location, **picks up the passenger**, drives to the passenger’s destination (another one of the four specified locations), and then **drops off the passenger**. Once the passenger is dropped off, the episode ends.\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL0wpeO8gpej"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBOaXgtsrmtT"
      },
      "source": [
        "There are **500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger** (including the case when the passenger is in the taxi), and **4 destination locations.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TPNaGSZrgqA"
      },
      "outputs": [],
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdeeZuokrhit"
      },
      "outputs": [],
      "source": [
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1r50Advrh5Q"
      },
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with **6 actions available 🎮**:\n",
        "\n",
        "- 0: move south\n",
        "- 1: move north\n",
        "- 2: move east\n",
        "- 3: move west\n",
        "- 4: pickup passenger\n",
        "- 5: drop off passenger\n",
        "\n",
        "Reward function 💰:\n",
        "\n",
        "- -1 per step unless other reward is triggered.\n",
        "- +20 delivering passenger.\n",
        "- -10 executing “pickup” and “drop-off” actions illegally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US3yDXnEtY9I"
      },
      "outputs": [],
      "source": [
        "# Create our Q table with state_size rows and action_size columns (500x6)\n",
        "Qtable_taxi = initialize_q_table(state_space, action_space)\n",
        "print(Qtable_taxi)\n",
        "print(\"Q-table shape: \", Qtable_taxi .shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUMKPH0_LJyH"
      },
      "source": [
        "## Define the hyperparameters ⚙️\n",
        "\n",
        "⚠ DO NOT MODIFY EVAL_SEED: the eval_seed array **allows us to evaluate your agent with the same taxi starting positions for every classmate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB6n__hhg7YS"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 25000   # Total training episodes\n",
        "learning_rate = 0.7           # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# DO NOT MODIFY EVAL_SEED\n",
        "eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
        " 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
        " 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n",
        "                                                          # Each seed has a specific starting state\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"Taxi-v3\"           # Name of the environment\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05           # Minimum exploration probability\n",
        "decay_rate = 0.005            # Exponential decay rate for exploration prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TMORo1VLTsX"
      },
      "source": [
        "## Train our Q-Learning agent 🏃"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwP3Y2z2eS-K"
      },
      "outputs": [],
      "source": [
        "Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n",
        "Qtable_taxi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPdu0SueLVl2"
      },
      "source": [
        "## Create a model dictionary 💾 and publish our trained model to the Hub 🔥\n",
        "\n",
        "- We create a model dictionary that will contain all the training hyperparameters for reproducibility and the Q-Table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a1FpE_3hNYr"
      },
      "outputs": [],
      "source": [
        "model = {\n",
        "    \"env_id\": env_id,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"n_training_episodes\": n_training_episodes,\n",
        "    \"n_eval_episodes\": n_eval_episodes,\n",
        "    \"eval_seed\": eval_seed,\n",
        "\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"gamma\": gamma,\n",
        "\n",
        "    \"max_epsilon\": max_epsilon,\n",
        "    \"min_epsilon\": min_epsilon,\n",
        "    \"decay_rate\": decay_rate,\n",
        "\n",
        "    \"qtable\": Qtable_taxi\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhQtiQozhOn1"
      },
      "outputs": [],
      "source": [
        "username = \"yanka9\" # FILL THIS\n",
        "repo_name = \"q-Taxi-v3\" # FILL THIS\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgSdjgbIpRti"
      },
      "source": [
        "Now that it's on the Hub, you can compare the results of your Taxi-v3 with your classmates using the leaderboard 🏆 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi-leaderboard.png\" alt=\"Taxi Leaderboard\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzgIO70c0bu2"
      },
      "source": [
        "# Part 3: Load from Hub 🔽\n",
        "\n",
        "What's amazing with Hugging Face Hub 🤗 is that you can easily load powerful models from the community.\n",
        "\n",
        "Loading a saved model from the Hub is really easy:\n",
        "\n",
        "1. You go https://huggingface.co/models?other=q-learning to see the list of all the q-learning saved models.\n",
        "2. You select one and copy its repo_id\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/copy-id.png\" alt=\"Copy id\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTth6thRoC6X"
      },
      "source": [
        "3. Then we just need to use `load_from_hub` with:\n",
        "- The repo_id\n",
        "- The filename: the saved model inside the repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtrfoTaBoNrd"
      },
      "source": [
        "#### Do not modify this code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo8qEzNtCaVI"
      },
      "outputs": [],
      "source": [
        "from urllib.error import HTTPError\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "def load_from_hub(repo_id: str, filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Download a model from Hugging Face Hub.\n",
        "    :param repo_id: id of the model repository from the Hugging Face Hub\n",
        "    :param filename: name of the model zip file from the repository\n",
        "    \"\"\"\n",
        "    # Get the model from the Hub, download and cache the model on your local disk\n",
        "    pickle_model = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=filename\n",
        "    )\n",
        "\n",
        "    with open(pickle_model, 'rb') as f:\n",
        "      downloaded_model_file = pickle.load(f)\n",
        "\n",
        "    return downloaded_model_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_sM2gNioPZH"
      },
      "source": [
        "### ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUm9lz2gCQcU"
      },
      "outputs": [],
      "source": [
        "model = load_from_hub(repo_id=\"ThomasSimonini/q-Taxi-v3\", filename=\"q-learning.pkl\") # Try to use another model\n",
        "\n",
        "print(model)\n",
        "env = gym.make(model[\"env_id\"])\n",
        "\n",
        "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7pL8rg1MulN"
      },
      "outputs": [],
      "source": [
        "model = load_from_hub(repo_id=\"ThomasSimonini/q-FrozenLake-v1-no-slippery\", filename=\"q-learning.pkl\") # Try to use another model\n",
        "\n",
        "env = gym.make(model[\"env_id\"], is_slippery=False)\n",
        "\n",
        "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQAwLnYFPk-s"
      },
      "source": [
        "## Some additional challenges 🏆\n",
        "\n",
        "The best way to learn **is to try things on your own**! As you saw, the current agent is not doing great. As a first suggestion, you can train for more steps. With 1,000,000 steps, we saw some great results!\n",
        "\n",
        "In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?\n",
        "\n",
        "Here are some ideas to climb up the leaderboard:\n",
        "\n",
        "* Train more steps\n",
        "* Try different hyperparameters by looking at what your classmates have done.\n",
        "* **Push your new trained model** on the Hub 🔥\n",
        "\n",
        "Are walking on ice and driving taxis too boring to you? Try to **change the environment**, why not use FrozenLake-v1 slippery version? Check how they work [using the gymnasium documentation](https://gymnasium.farama.org/) and have fun 🎉."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-fW-EU5WejJ"
      },
      "source": [
        "_____________________________________________________________________\n",
        "Congrats 🥳, you've just implemented, trained, and uploaded your first Reinforcement Learning agent.\n",
        "\n",
        "Understanding Q-Learning is an **important step to understanding value-based methods.**\n",
        "\n",
        "In the next Unit with Deep Q-Learning, we'll see that while creating and updating a Q-table was a good strategy — **however, it is not scalable.**\n",
        "\n",
        "For instance, imagine you create an agent that learns to play Doom.\n",
        "\n",
        "<img src=\"https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png\" alt=\"Doom\"/>\n",
        "\n",
        "Doom is a large environment with a huge state space (millions of different states). Creating and updating a Q-table for that environment would not be efficient.\n",
        "\n",
        "That's why we'll study Deep Q-Learning in the next unit, an algorithm **where we use a neural network that approximates, given a state, the different Q-values for each action.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjLhT70TEZIn"
      },
      "source": [
        "See you in Unit 3! 🔥\n",
        "\n",
        "## Keep learning, stay awesome 🤗"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "67OdoKL63eDD",
        "B2_-8b8z5k54",
        "8R5ej1fS4P2V",
        "Pnpk2ePoem3r"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
